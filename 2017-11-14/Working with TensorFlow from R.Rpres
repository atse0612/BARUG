Working with TensorFlow from R
========================================================
author: Joseph Rickert
date: November 14, 2017
autosize: true

This Talk
========================================================

[`TensorFlow`](https://github.com/tensorflow/tensorflow) has emerged as the premiere platform for Deep Learning and the production deployment of machine learning models.

[`Keras`](https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of [`TensorFlow`](https://github.com/tensorflow/tensorflow), [`CNTK`](https://github.com/Microsoft/cntk), or [`Theano`](https://github.com/Theano/Theano) written in Python. 

In this talk we will look at how RStudio is using `Keras` to provide access to `TensorFlow` Deep Learning Models from R.

TensorFlow
========================================================
- Although `TensorFlow` is associated with Deep Learning, it is a general purpose library for numerical computation that uses flow graphs.
- Nodes in a `TensorFlow` graph represent mathematical operations
- Data organized as multidimensional data arrays (tensors) that flow along the edges. 
- Edges may also indicate control signals that constrain the order of execution.
- The [paper](https://dl.acm.org/citation.cfm?doid=3088525.3088527) by Abadi, Isard and Murray from the Google Brain team that developed `TensorFlow` describes the computational model in some detail.

TensorFlow Paper
========================================================
![](TF_paper.png)


TensorFlow Graph
========================================================
![](TF_graph.png)

Getting TensorFlow
========================================================
- The first step is to install R package [keras](https://cran.r-project.org/package=keras) from CRAN
- The `install_keras()` function installs both `Keras` and `TensorFlow`
- The default is to install on your local machine
- The documentation for `install_keras` describes how to do custom installations (including using NVIDIA GPUs)

Installing TensorFlow
========================================================
The `install_keras()` function installs `Keras`, `TensorFlow` and all of the necessary `Python` machinery.

```{r,eval=FALSE}
#install.packages("keras")
#library(keras)
#install_keras()   
````   

The MNIST Example
=========================================================
The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset contains: 
- 60,000 28x28 grayscale images of the 10 digits
- Labels for each image
- An additional test set of 10,000 images.
- Is used to train image processing systems 
- Is included in the `keras` package. 

Typical Grayscale Image
==========================================================
A typical MNIST grayscale image looks something like this:

![](MNIST.png)   

Load the Keras Library and look at MNIST
===========================================================
```{r, message=FALSE}
library(keras)
mnist <- dataset_mnist()
str(mnist,give.attr = FALSE)
```    

Organize Data for TensorFlow
===========================================================
Here, we organize the data for conveniently feeding it to `TensorFlow`. The x data is a 3-d array (images,width,height) of grayscale values.

```{r}    
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

Reshape for Training
===========================================================
- Convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors)
- Convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1. 
- The y data is an integer vector with values ranging from 0 to 9
- [one-hot encode](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science) the vectors into binary class matrices using the function `keras::to_categorical()`.

Reshape the Data     
===========================================================

Note: the `array_reshape()` function and not an R function is used to reshape the array. This is so that the data is re-interpreted using row-major semantics (as opposed to Râ€™s default column-major semantics), which is compatible with the way that the numerical libraries called by Keras.

```{r}
# reshape
dim(x_train) <- c(nrow(x_train), 784)
dim(x_test) <- c(nrow(x_test), 784)
# rescale
x_train <- x_train / 255
x_test <- x_test / 255

y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
``` 

Define the Keras Model
==============================================================
In Keras, data is organized into layers. A model in Keras is a collection of layers. The simplest type of model is the [sequential model](https://keras.rstudio.com/articles/sequential_model.html) which is a linear stack of layers.

- Begin by creating a sequential model using the sequential model constructor `keras_model_sequential` 
- Then adding layers using the pipe `%>%` operator 
- `layer_dense()` adds a densely-connected NN layer

Create Sequential Model:
================================================================
```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = "relu", input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = "softmax")
```


Arguments to `keras_model_sequential`
================================================================
- The `units` argument defines the dimension of the output space 
- `activation` = "relu" indicates that nodes in this layer will be [activated](https://en.wikipedia.org/wiki/Activation_function) as rectified linear units, [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))

- `input_shape` specifies the dimensionality of the input 784 is the length of the grayscale image 
- rate = .4 in `layer_dropout()` means that 40% of the units will randomly be set to 0 during the training. This is a technique to prevent overfitting
- The final layer outputs a length 10 numeric vector (probabilities for each digit) using a [softmax activation](https://en.wikipedia.org/wiki/Softmax_function) function

Summarize the model
=================================================================
The 200,960 parameters in the first layer comes from (784 inputs) * (256 neurons) + (256 bias values).

```{r}   
#summary(model)
```
![](summary.png) 

Compile the Model
==================================================================
Before training a keras model you need to configure the learning process. This done with the [`compile`](https://faroit.github.io/keras-docs/1.0.1/getting-started/sequential-model-guide/) function.

```{r}
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy"))
```

Compile Parameters
====================================================================

- `loss` specifies the loss function that the model will attempt to minimize. Look [here](https://en.m.wikipedia.org/wiki/Cross_entropy) for a discussion of categorical cross entropy. 
- `optimizer` specifies the optimization algorithm to be used. In this case, we use the [remsprop](http://climin.readthedocs.io/en/latest/rmsprop.html) optimizer which uses the magnitude of recent gradients to normalize gradients. The [keras documentation](https://keras.io/optimizers/) suggests that this is usually a good choice for recurrent neural networks. 
- Look [here](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) to see how `TensorFlow` calculates accuracy.

Fit the Model
====================================================================

We fit the model for 30 epochs using batches of 128 images and specifying that 20% of the training data will be used for validation.
```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```   

Look at Model Object
=====================================================================

```{r}
str(history)
```    

Plot the Model Fit
=======================================================================
Plot accuracy and loss for both the training and validation for 30 epochs.
```{r}
plot(history)
```  

Evaluate Model Performance
=========================================================================
```{r}
model %>% evaluate(x_test, y_test,verbose = 0)
```

Generate Predictions
=========================================================================
The `predict_classes()` function can be used to generate predictions for new data.
```{r}
model %>% predict_classes(x_test)
```

